# Optimized Configuration for Effective Fine-tuning
# Designed to make the model learn new preferences more effectively

# Model Configuration
model:
  name: "meta-llama/Llama-3.2-1B-Instruct"
  max_length: 2048
  trust_remote_code: false

# LoRA Configuration - Increased capacity for better learning
lora:
  r: 64                    # Increased from 16 for more capacity
  lora_alpha: 128          # Increased from 32 (should be 2x rank)
  lora_dropout: 0.1        # Keep dropout for regularization
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Training Configuration - More aggressive learning
training:
  learning_rate: 1e-3      # Increased from 2e-4 (5x higher)
  num_train_epochs: 5      # Increased from 3 for better convergence
  per_device_train_batch_size: 2    # Reduced for more frequent updates
  gradient_accumulation_steps: 8    # Increased to maintain effective batch size
  warmup_steps: 50         # Reduced warmup for faster learning
  weight_decay: 0.01       # Keep weight decay
  max_grad_norm: 1.0       # Keep gradient clipping
  save_steps: 100          # More frequent checkpoints
  eval_steps: 100          # More frequent evaluation
  logging_steps: 5         # More frequent logging
  save_total_limit: 5      # Keep more checkpoints

# Data Configuration
data:
  max_seq_length: 2048
  conversation_length_limit: 10

# Output Configuration
output:
  model_dir: "./models/cats_optimized2"
  checkpoint_dir: "./models/checkpoints_optimized"
  logs_dir: "./logs"

# Evaluation Configuration
evaluation:
  metrics: ["bleu", "rouge", "exact_match"]
  generation:
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9
    do_sample: true
